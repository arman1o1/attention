{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Translation with Transformers\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In the previous notebook, we built the Encoder-Decoder architecture and trained it on a tiny synthetic dataset. Now, we will apply that same architecture to a **real-world translation task**.\n",
    "\n",
    "We will build an **English-to-French translator**.\n",
    "\n",
    "### What's New in This Notebook?\n",
    "\n",
    "1.  **Real Data Pipeline**: Downloading, cleaning, and processing raw text files.\n",
    "2.  **Vocabulary Building**: Mapping real words to indices based on frequency.\n",
    "3.  **Batching & Padding**: Handling sentences of different lengths using `pad_sequence` and `collate_fn`.\n",
    "4.  **Inference**: Translating unseen English sentences.\n",
    "\n",
    "### The Dataset\n",
    "We will use the English-French dataset from [Tatoeba](https://tatoeba.org/), hosted by [ManyThings.org](http://www.manythings.org/anki/). It contains sentence pairs ranging from very simple (\"Go.\") to complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Setup Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Cleaning\n",
    "\n",
    "First, we download the dataset directly from the source. The file is a tab-separated text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Extracting dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def download_data():\n",
    "    url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "    filename = \"fra-eng.zip\"\n",
    "    \n",
    "    if not os.path.exists(\"fra.txt\"):\n",
    "        print(\"Downloading dataset...\")\n",
    "        \n",
    "        # Create a request with a browser-like User-Agent header\n",
    "        req = urllib.request.Request(\n",
    "            url, \n",
    "            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        )\n",
    "        \n",
    "        # Download using urlopen instead of urlretrieve to support headers easily\n",
    "        with urllib.request.urlopen(req) as response, open(filename, 'wb') as out_file:\n",
    "            data = response.read()\n",
    "            out_file.write(data)\n",
    "            \n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Utils\n",
    "\n",
    "Real text is messy. We need to:\n",
    "1.  **Unicode Normalization**: Convert accented characters to a standard form.\n",
    "2.  **Clean**: Remove non-alphabetic characters (keep punctuation separate).\n",
    "3.  **Tokenize**: Split sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 50000 sentence pairs.\n",
      "Sample: ['go now .', 'allez y maintenant .']\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # Add space between punctuation and words (e.g., \"hi!\" -> \"hi !\")\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # Remove anything that isn't a letter or punctuation\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def read_text_file(filename, limit=None):\n",
    "    print(\"Reading lines...\")\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    \n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        # File format: English \\t French \\t Attribution\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            eng = normalize_string(parts[0])\n",
    "            fra = normalize_string(parts[1])\n",
    "            pairs.append([eng, fra])\n",
    "            \n",
    "    # Limit data for faster training in this tutorial\n",
    "    if limit:\n",
    "        pairs = pairs[:limit]\n",
    "        \n",
    "    print(f\"Read {len(pairs)} sentence pairs.\")\n",
    "    return pairs\n",
    "\n",
    "# Let's load 50,000 sentences (enough for a good demo)\n",
    "pairs = read_text_file(\"fra.txt\", limit=50000)\n",
    "print(\"Sample:\", pairs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Building\n",
    "\n",
    "We need to convert words into numbers (indices). We'll create a `Vocabulary` class that:\n",
    "1.  Assigns unique IDs to words.\n",
    "2.  Handles special tokens: `<pad>`, `<sos>`, `<eos>`, `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size: 5865\n",
      "French Vocab Size: 9942\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        self.n_words = 4  # Start count\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "    def sentence_to_indices(self, sentence):\n",
    "        return [self.word2index.get(word, 3) for word in sentence.split(' ')] # 3 is <unk>\n",
    "\n",
    "# Build vocabularies\n",
    "input_vocab = Vocabulary()\n",
    "output_vocab = Vocabulary()\n",
    "\n",
    "for pair in pairs:\n",
    "    input_vocab.add_sentence(pair[0])\n",
    "    output_vocab.add_sentence(pair[1])\n",
    "\n",
    "print(f\"English Vocab Size: {input_vocab.n_words}\")\n",
    "print(f\"French Vocab Size: {output_vocab.n_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Batching\n",
    "\n",
    "Unlike the synthetic example where all sentences were the same length, real sentences vary. We must:\n",
    "1.  Convert sentences to tensors.\n",
    "2.  **Pad** shorter sentences in a batch to match the longest one.\n",
    "3.  Use a `collate_fn` in the DataLoader to handle this padding dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shapes: torch.Size([64, 7]) torch.Size([64, 11])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, input_vocab, output_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_text, fra_text = self.pairs[idx]\n",
    "        \n",
    "        # Convert to indices\n",
    "        eng_indices = self.input_vocab.sentence_to_indices(eng_text)\n",
    "        fra_indices = self.output_vocab.sentence_to_indices(fra_text)\n",
    "        \n",
    "        # Add <sos> and <eos>\n",
    "        # Src: [word, word, ...]\n",
    "        # Tgt: [<sos>, word, word, ..., <eos>]\n",
    "        # (We don't strictly need <sos>/<eos> on source for this model, but usually helpful)\n",
    "        \n",
    "        eng_tensor = torch.tensor(eng_indices, dtype=torch.long)\n",
    "        fra_tensor = torch.tensor([1] + fra_indices + [2], dtype=torch.long)\n",
    "        \n",
    "        return eng_tensor, fra_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Sort batch by source length (helps with efficiency, though not strictly required)\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    # padding_value=0 is our <pad> token\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# Create DataLoaders\n",
    "dataset = TranslationDataset(pairs, input_vocab, output_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Check one batch\n",
    "src_sample, tgt_sample = next(iter(dataloader))\n",
    "print(\"Batch Shapes:\", src_sample.shape, tgt_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Model (Transformer)\n",
    "\n",
    "We reuse the Encoder-Decoder architecture. \n",
    "\n",
    "**Key Details:**\n",
    "-   **Positional Encoding**: Adds order to the sequences.\n",
    "-   **Encoder**: Processes the padded source batch.\n",
    "-   **Decoder**: Processes the target batch with masking.\n",
    "-   **Masks**: We need to handle the padding (0s) so the model ignores them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=4, \n",
    "                 num_layers=2, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_layers, \n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.out_fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    def create_src_mask(self, src):\n",
    "        # Mask: True where value is 0 (padding)\n",
    "        return (src == 0)\n",
    "    \n",
    "    def create_tgt_mask(self, tgt):\n",
    "        # Padding mask\n",
    "        tgt_pad_mask = (tgt == 0)\n",
    "        \n",
    "        # Causal mask (prevent looking forward)\n",
    "        sz = tgt.size(1)\n",
    "        tgt_mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1)\n",
    "        tgt_mask = tgt_mask.to(tgt.device)\n",
    "        \n",
    "        return tgt_mask, tgt_pad_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_key_padding_mask = self.create_src_mask(src)\n",
    "        tgt_mask, tgt_key_padding_mask = self.create_tgt_mask(tgt)\n",
    "        \n",
    "        # Embeddings + Positional\n",
    "        src_emb = self.pos_encoder(self.embedding_src(src))\n",
    "        tgt_emb = self.pos_encoder(self.embedding_tgt(tgt))\n",
    "        \n",
    "        # Transformer Pass\n",
    "        # Note: memory_key_padding_mask tells decoder to ignore source padding\n",
    "        outs = self.transformer(\n",
    "            src=src_emb, \n",
    "            tgt=tgt_emb, \n",
    "            src_key_padding_mask=src_key_padding_mask, \n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.out_fc(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "We will train the model. Note that we use `ignore_index=0` in the loss function so that the model isn't penalized for predicting the wrong thing in padded positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 50000 sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 3.8314\n",
      "Epoch 2/10 | Loss: 2.6951\n",
      "Epoch 3/10 | Loss: 2.2988\n",
      "Epoch 4/10 | Loss: 2.0161\n",
      "Epoch 5/10 | Loss: 1.7894\n",
      "Epoch 6/10 | Loss: 1.6052\n",
      "Epoch 7/10 | Loss: 1.4514\n",
      "Epoch 8/10 | Loss: 1.3193\n",
      "Epoch 9/10 | Loss: 1.2052\n",
      "Epoch 10/10 | Loss: 1.1077\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "SRC_VOCAB = input_vocab.n_words\n",
    "TGT_VOCAB = output_vocab.n_words\n",
    "EPOCHS = 10  # Increase this for better results (e.g., 20-30)\n",
    "\n",
    "model = TransformerModel(SRC_VOCAB, TGT_VOCAB).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignore <pad>\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "print(f\"Training on {len(pairs)} sentences...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # tgt_input: <sos> ... words\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        # tgt_output: words ... <eos>\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # Output: [batch, seq_len, vocab_size] -> [batch*seq_len, vocab_size]\n",
    "        loss = criterion(output.reshape(-1, TGT_VOCAB), tgt_output.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Prevent exploding grads\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference / Translation\n",
    "\n",
    "Now we can try to translate new sentences. The process is:\n",
    "1. Tokenize input string.\n",
    "2. Feed to Encoder.\n",
    "3. Autoregressively generate the Output (start with `<sos>`, keep predicting until `<eos>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Translations ---\n",
      "En: I am happy.\n",
      "Fr: je suis heureux .\n",
      "--------------------\n",
      "En: She is my friend.\n",
      "Fr: elle est mon amie .\n",
      "--------------------\n",
      "En: Where are you going?\n",
      "Fr: ou vas tu ?\n",
      "--------------------\n",
      "En: This is a book.\n",
      "Fr: c est un livre .\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, model, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess input\n",
    "    sentence = normalize_string(sentence)\n",
    "    src_indices = input_vocab.sentence_to_indices(sentence)\n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get Encoder memory\n",
    "    src_mask = model.create_src_mask(src_tensor)\n",
    "    src_emb = model.pos_encoder(model.embedding_src(src_tensor))\n",
    "    memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_mask)\n",
    "    \n",
    "    # Start decoder with <sos>\n",
    "    tgt_indices = [1]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Create masks for decoder\n",
    "        tgt_mask, _ = model.create_tgt_mask(tgt_tensor)\n",
    "        \n",
    "        tgt_emb = model.pos_encoder(model.embedding_tgt(tgt_tensor))\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        out = model.transformer.decoder(\n",
    "            tgt_emb, \n",
    "            memory, \n",
    "            tgt_mask=tgt_mask, \n",
    "            memory_key_padding_mask=src_mask\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        out = model.out_fc(out)\n",
    "        \n",
    "        # Get last token probability\n",
    "        prob = out[0, -1, :]\n",
    "        next_token = torch.argmax(prob).item()\n",
    "        \n",
    "        if next_token == 2: # <eos>\n",
    "            break\n",
    "        \n",
    "        tgt_indices.append(next_token)\n",
    "    \n",
    "    # Convert indices to words\n",
    "    translated_words = [output_vocab.index2word.get(idx, \"\") for idx in tgt_indices[1:]]\n",
    "    return \" \".join(translated_words)\n",
    "\n",
    "# Test on some sentences from the dataset (or new ones)\n",
    "test_sentences = [\n",
    "    \"I am happy.\",\n",
    "    \"She is my friend.\",\n",
    "    \"Where are you going?\",\n",
    "    \"This is a book.\"\n",
    "]\n",
    "\n",
    "print(\"--- Translations ---\")\n",
    "for s in test_sentences:\n",
    "    print(f\"En: {s}\")\n",
    "    print(f\"Fr: {translate(s, model)}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "You have successfully trained a Transformer on a real translation dataset!\n",
    "\n",
    "### Observations\n",
    "1.  **Overfitting**: With 50k sentences and a small model, it might memorize common phrases nicely but struggle with complex grammar. More data + Dropout helps.\n",
    "2.  **Training Time**: Real vocabularies (thousands of words) make the final Linear layer larger, increasing computation compared to synthetic examples.\n",
    "3.  **Preprocessing**: Much of the work in real-world NLP is just getting the data cleaned, tokenized, and batched correctly.\n",
    "\n",
    "### Next Steps to Improve\n",
    "-   **Use Subword Tokenization**: We used simple whitespace splitting. Modern models use BPE (Byte Pair Encoding) or WordPiece to handle unknown words and morphology better.\n",
    "-   **Beam Search**: Replace the \"Argmax\" in the translation loop with Beam Search to find better translations.\n",
    "-   **Evaluation Metric**: Implement BLEU score to quantitatively measure translation quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
