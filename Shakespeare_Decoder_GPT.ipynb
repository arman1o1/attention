{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT-Style Decoder from Scratch\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we will demystify **Decoder-Only Models**. These are the architectures behind the generative AI revolution (GPT-3, ChatGPT, LLaMA, Claude).\n",
    "\n",
    "While Encoders (like BERT) are great at *understanding* text, Decoders are built for *generation*. They predict the next token in a sequence based on the history.\n",
    "\n",
    "### The Goal\n",
    "We will build a model that reads the works of **William Shakespeare** and learns to generate new plays in his style, character by character.\n",
    "\n",
    "### Key Concepts You Will Implement:\n",
    "1.  **Causal Masking**: The \"secret sauce\" that prevents the model from cheating by looking into the future.\n",
    "2.  **The Decoder Block**: Combining Masked Self-Attention with Feed-Forward networks.\n",
    "3.  **Autoregressive Generation**: The loop of predicting a token, adding it to the input, and predicting again.\n",
    "4.  **Temperature Sampling**: Controlling the creativity of the AI.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Causal Mask (The \"No Peeking\" Rule)\n",
    "\n",
    "The most critical difference between a standard Attention block and a Decoder is the **Causal Mask**.\n",
    "\n",
    "When the model tries to predict the word at position 4, it should see positions 1, 2, and 3. It should **not** see position 5. If it sees position 5, it's not predicting; it's cheating.\n",
    "\n",
    "We achieve this by setting the attention scores of future tokens to `-infinity`. When we apply Softmax, these become `0`.\n",
    "\n",
    "**Visualizing the Mask:**\n",
    "We want a triangular matrix where the upper right is blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Mask Values:\n",
      " tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def generate_causal_mask(sz):\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag and below.\n",
    "    Shape: [sz, sz]\n",
    "    \"\"\"\n",
    "    # Create a mask with 1s in the upper triangle\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "    \n",
    "    # Replace 1s with -inf, and 0s with 0\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Let's visualize a mask for a sentence of 5 words\n",
    "seq_len = 5\n",
    "mask = generate_causal_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(mask == 0, cmap='Blues') # Plotting boolean for visibility\n",
    "plt.title(\"Causal Mask (Blue = Visible, White = Hidden)\")\n",
    "plt.ylabel(\"Current Position\")\n",
    "plt.xlabel(\"Attending To\")\n",
    "plt.xticks(range(seq_len))\n",
    "plt.yticks(range(seq_len))\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Actual Mask Values:\\n\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation: Tiny Shakespeare\n",
    "\n",
    "We will use a character-level tokenizer. This means our \"vocabulary\" is just the list of unique characters (letters, punctuation) in the text. This is simpler than word-level tokenization and allows the model to create new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394 characters\n",
      "Vocabulary size: 65\n",
      "Vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Encoded 'hello': [46, 43, 50, 50, 53]\n",
      "Decoded: hello\n"
     ]
    }
   ],
   "source": [
    "# 1. Download the data\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "data = requests.get(url).text\n",
    "\n",
    "print(f\"Length of dataset: {len(data)} characters\")\n",
    "\n",
    "# 2. Build Vocabulary\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocab: {''.join(chars)}\")\n",
    "\n",
    "# 3. Mappings\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # String to Index\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # Index to String\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test\n",
    "print(f\"\\nEncoded 'hello': {encode('hello')}\")\n",
    "print(f\"Decoded: {decode(encode('hello'))}\")\n",
    "\n",
    "# 4. Convert entire text to tensor\n",
    "data = torch.tensor(encode(data), dtype=torch.long)\n",
    "train_data = data[:int(0.9*len(data))] # 90% Train\n",
    "val_data = data[int(0.9*len(data)):]   # 10% Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Batch Generation\n",
    "\n",
    "In a decoder, **Input** is `Sequence[0 -> T-1]`, and **Target** is `Sequence[1 -> T]`.\n",
    "\n",
    "Example Sentence: \"HELLO\"\n",
    "- Input: \"HELL\"\n",
    "- Target: \"ELLO\" \n",
    "\n",
    "The model learns: given 'H', predict 'E'; given 'HE', predict 'L', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([32, 64])\n",
      "Targets shape: torch.Size([32, 64])\n",
      "Input example: Paris!\n",
      "What said my man, when my betossed soul\n",
      "Did not attend hi\n",
      "Target example: aris!\n",
      "What said my man, when my betossed soul\n",
      "Did not attend him\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # How many independent sequences will we process in parallel?\n",
    "block_size = 64 # What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Select data source\n",
    "    data_source = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Generate random starting positions\n",
    "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
    "    \n",
    "    # Stack inputs (x) and targets (y)\n",
    "    x = torch.stack([data_source[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs shape:\", xb.shape)\n",
    "print(\"Targets shape:\", yb.shape)\n",
    "print(f\"Input example: {decode(xb[0].tolist())}\")\n",
    "print(f\"Target example: {decode(yb[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Model Architecture\n",
    "\n",
    "We will build a **Transformer Decoder**. \n",
    "\n",
    "Technically, in PyTorch, the standard `TransformerDecoder` includes Cross-Attention (for translation). Since we are doing **GPT-style generation** (just text generation, no translation source), we actually use `TransformerEncoderLayer` logic but apply the **Causal Mask**.\n",
    "\n",
    "To make this crystal clear, we will wrap the logic in a clean `DecoderOnlyTransformer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Standard sinusoidal positional encoding\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing position\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a learnable parameter, but part of state_dict)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, SeqLen, HiddenDim]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class GPTDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_len=block_size)\n",
    "        \n",
    "        # 2. The Transformer Layers\n",
    "        # We use TransformerEncoderLayer because we only need Self-Attention\n",
    "        # The \"Decoder\" aspect comes from the MASK we pass during forward()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True # Expect [Batch, Seq, Feature]\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 3. Final Norm & Output Head\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # 1. Embed Token + Position\n",
    "        x = self.token_embedding(idx)\n",
    "        x = self.position_encoding(x)\n",
    "\n",
    "        # 2. Create Causal Mask\n",
    "        # The mask must be on the same device as the input\n",
    "        mask = generate_causal_mask(T).to(idx.device)\n",
    "\n",
    "        # 3. Pass through Transformer with Mask\n",
    "        # is_causal=True is a hint for optimization in newer PyTorch, \n",
    "        # but passing mask explicitly is the classic robust way.\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # 4. Project to Vocab Size\n",
    "        logits = self.head(x) # Shape: [Batch, SeqLen, VocabSize]\n",
    "\n",
    "        # 5. Calculate Loss (if targets provided)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Flatten predictions and targets for CrossEntropy\n",
    "            # Logits: [B*T, Vocab], Targets: [B*T]\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "We will initialize a small model (for speed) and train it. With a GPU, this should take a minute or two to start seeing recognizable English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3.19M parameters\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "d_model = 256     # Hidden dimension size\n",
    "nhead = 4         # Number of attention heads\n",
    "num_layers = 4    # Number of transformer blocks\n",
    "learning_rate = 3e-4\n",
    "max_iters = 2000\n",
    "eval_interval = 200\n",
    "\n",
    "model = GPTDecoder(vocab_size, d_model, nhead, num_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step 0: Loss 4.4646\n",
      "Step 200: Loss 2.2433\n",
      "Step 400: Loss 2.0711\n",
      "Step 600: Loss 1.9214\n",
      "Step 800: Loss 1.8073\n",
      "Step 1000: Loss 1.7586\n",
      "Step 1200: Loss 1.7676\n",
      "Step 1400: Loss 1.6728\n",
      "Step 1600: Loss 1.6746\n",
      "Step 1800: Loss 1.5758\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    if iter % eval_interval == 0:\n",
    "        print(f\"Step {iter}: Loss {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation: Creating New Text\n",
    "\n",
    "Unlike training (where we do everything in parallel), generation is a loop:\n",
    "1. Feed current sequence.\n",
    "2. Get logit for the *last* token.\n",
    "3. Apply Softmax to get probabilities.\n",
    "4. **Sample** a new index based on probabilities.\n",
    "5. Append new index to sequence.\n",
    "6. Repeat.\n",
    "\n",
    "### Temperature\n",
    "We use `temperature` to control randomness.\n",
    "- `temp < 1`: Makes the distribution sharper (model becomes more confident/conservative).\n",
    "- `temp > 1`: Flattens the distribution (model takes more risks/is more creative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_str, max_new_tokens=200, temperature=1.0):\n",
    "    # 1. Encode starting string to tensor\n",
    "    idx = torch.tensor(encode(start_str), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Cropping: If sequence exceeds block_size, keep only the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = model(idx_cond)\n",
    "            \n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Shakespeare (Temp 0.8) ---\n",
      "ROMEO: I crome to go Romeo, sir.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "What could lost do much for the farewter to bless\n",
      "your ender time, is he loved any coundrat thou art\n",
      "Are for speaks, sir.\n",
      "\n",
      "ROMEO:\n",
      "Here do mistress and light, parging at despare what\n",
      "Shall be a leing to this first a baning my good\n",
      "this never bower than tway \n",
      "\n",
      "--- Generated Shakespeare (Temp 0.2 - Conservative) ---\n",
      "ROMEO: I have we so shall be the some of the brother,\n",
      "And the speak of the fair of the soul,\n",
      "And by the some of the stand of the cannot\n",
      "That be the stand of the brother to be the country.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "What shall be the more to me to so my lord.\n",
      "\n",
      "KING EDWARD IV:\n",
      "What I will not the some of the commmon o\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Generated Shakespeare (Temp 0.8) ---\")\n",
    "print(generate_text(model, start_str=\"ROMEO: \", max_new_tokens=300, temperature=0.8))\n",
    "\n",
    "print(\"\\n--- Generated Shakespeare (Temp 0.2 - Conservative) ---\")\n",
    "print(generate_text(model, start_str=\"ROMEO: \", max_new_tokens=300, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "You have successfully built a generative Transformer from scratch!\n",
    "\n",
    "### Key Takeaways:\n",
    "1.  **Decoders generate autoregressively**: They need the immediate past to predict the immediate future.\n",
    "2.  **Masking is crucial**: The Causal Mask ensures the logic holds during parallel training.\n",
    "3.  **Scale matters**: We used a tiny model (~1M params). GPT-3 has 175 Billion. The architecture is nearly identical; the difference is data and size.\n",
    "\n",
    "**Extensions:**\n",
    "- Try increasing `num_layers` and `d_model` and training for longer.\n",
    "- Try implementing \"Top-K\" sampling in the generation function to prevent the model from picking very unlikely words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
